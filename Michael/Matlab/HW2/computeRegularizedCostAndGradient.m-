function [J, Gradient] = computeRegularizedCostAndGradient (D, Y, Hypothesis,lambda)
%UNTITLED Summary of this function goes here
%   Detailed explanation goes here
predictVal=predictValue(D,Hypothesis);
Gradient=zeros(1,size(D,2));
J=0; 
eps=0.0001;
theta=0;
gradTheta=0;

for i=1:size(predictVal,1)
    if(predictVal(i)==0)
        J=J+(-Y(i)*log(predictVal(i)+eps)-((1-Y(i))*log(1-predictVal(i))));  
    else if(predictVal(i)==1)
        J=J+(-Y(i)*log(predictVal(i))-((1-Y(i))*log(1-(predictVal(i)-eps))));
    else
        J=J+(-Y(i)*log(predictVal(i))-((1-Y(i))*log(1-predictVal(i))));
        end 
    end
end

for i=1:size(predictVal,1)
    error=predictVal(i)-Y(i);
    Gradient=Gradient+(error*D(i,:));
end

J=J/size(D,1);
Gradient= Gradient/size(D,1);

for j=1:size(D,2)
    theta= theta + Hypothesis(j)*Hypothesis(j);
end

J=J+((lambda/(2*size(predictVal,1)))*theta);

for k=2:size(D,2)
    gradTheta= gradTheta + Hypothesis(j);
end
Gradient= Gradient+((Lambda/


end





